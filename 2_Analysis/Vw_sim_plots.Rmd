---
title: "Vw_sim_plots"
author: "Manas GA"
date: "2024-12-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries}
library(ggplot2)
library(cowplot)
library(latex2exp)
```

The following code loads outputs of simulation analyses and makes Figures for the manuscript. Simulation data is classified into sets, with the sets that are useful for the manuscript being  as follows (the number of simulations within each set are indicated inside brackets):

- Set_5 (900 simulations): All simplified simulations 
- Set_9 (100 simulations, each analysed in three different ways): The standard set for full simulations 
- Set_11 (100): Full simulations with 0.02 % beneficial mutations
- Set_14 (100): Full simulations with 2 % beneficial mutations
- Set_12 (200): Full simulations varying map length in th experiment
- Set_13 (176): Full simulations varying map length in history
- Set_15 (200): Full simulations varying the scale
- Set_N1 (396): Full simulations varying the map length in history (for simulations in which scale = 0.045)
- sanity_check (80): Simplified simulations with nearly free recombination in the experiment

Total = 2252



## Set paths

```{r paths}

### On Manas's PC ###

# EdiUni Datastore needs to be mounted via a vpn 
# Or works only on the University's network

if(Sys.info()["nodename"]=="SCE-BIO-C06645"){

  if(Sys.info()["sysname"]=="Linux"){
    
    data_path = "/mnt/u/Datastore/CSCE/biology/groups/hadfield/Va_simulations/4_analysed_data" ## Local Wsl
    output_path = "/mnt/c/Users/msamant/Documents/GitHub/Va_simulations/5_Manuscript_files/Manuscript_figures"
    
  }else{
    
    data_path = "U:/Datastore/CSCE/biology/groups/hadfield/Va_simulations/4_analysed_data" ## Local windows
    output_path = "C:/Users/msamant/Documents/GitHub/Va_simulations/5_Manuscript_files/Manuscript_figures"
  }
}


### On Jarrod's PC ###

# EdiUni Datastore needs to be mounted via a vpn 
# Or works only on the University's network

if(Sys.info()["nodename"]=="sce-bio-c04553"){  
  data_path = "/Volumes/hadfield/Va_simulations/4_analysed_data"
  output_path = "~/Work/Va_simulations/5_Manuscript_files/Manuscript_figures"
}

```

## Function to remove duplication

Some simulations were erroneously analysed twice. The following function takes a data frame for a set of simulations as input, and if there are any simulations analysed twice, retains the analysis performed earlier in time.

```{r}

deduplicate_sim_analyses = function(data_frame){

  for (set_id in unique(data_frame$Set_ID)){
    
    count = sum(data_frame$Set_ID == set_id)
    
    if(count>1){
      data_sim = data_frame[data_frame$Set_ID==set_id,]
      
      # Remove the nodename and the year from the analysis stamp and then identify the analysis which was performed later 
      
      max_time = max(sub(".*_2025-", "",  data_sim$analysis_stamp))
      analysis_to_be_deleted = data_sim$analysis_stamp[grep(max_time, data_sim$analysis_stamp)]
      
      # Delete this analysis from d_9_nstd
      
      data_frame = data_frame[data_frame$analysis_stamp!=analysis_to_be_deleted,]
      
    }
    
  }
  
  return(data_frame)
  
}
```

## Function to add recalculated true V_A (V_A_true_new), lost_va to a dataframe

These calculations were performed at a later stage and are stored in a separate file ("<data_path>/combined_data/new_va_calculation_combined.csv"). For every simulation in the input dataframe this functions looks up the row with the appropriate Set_ID in "new_va_calculation_combined.csv" and adds the V_A_true_new and other data pertaining to lost_va to the input dataframe.

```{r}
add_new_va_data = function(new_va_data,    # Data frame storing the calculations of vA_true_new, lost_va, etc
                           target_data,    # Target dataframe to which this information is to be added
                           verbose = FALSE){   
  
  # Create empty columns in the target data
  
  target_data$vA_true_new = NA
  target_data$va_true_new = NA
  target_data$va_left = NA
  target_data$va_left_new = NA
  
  for(set_id in target_data$Set_ID){
    
    # Check if the set_id exists in new_va_data
  
   if(!set_id%in%new_va_data$Set_ID){
     if(verbose){warning(paste("Simulation ", set_id, " is missing from new_va_calculation_combined.csv", sep = ""))}
   }else{
     
      # Add data to target_data
      target_data[target_data$Set_ID == set_id,]$vA_true_new = new_va_data[new_va_data$Set_ID == set_id,]$vA_true_new
      target_data[target_data$Set_ID == set_id,]$va_true_new = new_va_data[new_va_data$Set_ID == set_id,]$va_true_new
      target_data[target_data$Set_ID == set_id,]$va_left = new_va_data[new_va_data$Set_ID == set_id,]$va_left
      target_data[target_data$Set_ID == set_id,]$va_left_new = new_va_data[new_va_data$Set_ID == set_id,]$va_left_new
       
     }
    
    
    
  }
  return(target_data)
}
```


## Function to make "standard" plot

Creates plots with four panels:
A. Estimate of V_A vs True V_A
B. Hisotgram of the estimates pf p_{\alpha}
C. Histogram of the estimates of (\beta)_{\alpha}^{(0)}
D. Histogram of the estimates of (\beta)_{\alpha}^{(1)}

```{r}
make_std_plots = function(data, 
                          alpha = FALSE, # If alpha = TRUE, alpha = eta + (1 - 2p)eta is used to calculate vA_true, i.e. vA_true_new is used in the plots. Otherwise, vA_true is used in the plots
                          font_size = 15, 
                          end_gen = 25000, 
                          data_in_title = FALSE){
  
  # vA_est ~ vA_true

p_main = ggplot(data, aes(y = vA_est, x = if(alpha){vA_true_new}else{vA_true})) + 
  theme_bw() + 
  geom_point() +
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)")) + 
  theme(text = element_text(size = font_size)) 
  
# palpha_est histogram

p_palpha = ggplot(data, aes(x = palpha_est)) + 
  theme_bw() + 
  geom_histogram(color = "black", fill = "white") + 
  labs(x = TeX(r"(Estimate of $p_{\alpha} $)"), y = "Frequency") + 
  geom_vline(xintercept = 0, color = "red") +  
  geom_vline(xintercept = mean(data$palpha_est), linetype = "dashed") +
  theme(text = element_text(size = font_size)) 

# balpha_intercept_est histogram

p_balpha_int = ggplot(data, aes(x = balpha_intercept_est)) + 
  theme_bw() + 
  geom_histogram(color = "black", fill = "white") + 
  labs(x = TeX(r"(Estimate of $\beta^{(0)}_{\bar{\alpha}}$)"), y = "Frequency") + 
  geom_vline(xintercept = 0, color = "red") + 
  geom_vline(xintercept = mean(data$balpha_intercept_est), linetype = "dashed") +
  theme(text = element_text(size = font_size)) 

# balpha_slope_est histogram

p_balpha_slope = ggplot(data, aes(x = balpha_slope_est)) + 
  theme_bw() + 
  geom_histogram(color = "black", fill = "white") + 
  labs(x = TeX(r"(Estimate of $\beta^{(1)}_{\bar{\alpha}}$)"), y = "Frequency") + 
  geom_vline(xintercept = 0, color = "red") + 
  geom_vline(xintercept = mean(data$balpha_slope_est), linetype = "dashed") +
  theme(text = element_text(size = font_size))

# Combine the four plots

p_combined = plot_grid(p_main, p_palpha, p_balpha_int, p_balpha_slope, labels = "AUTO")

if(data_in_title){

  # Title
  
  ml = unique(data$sequence_length*data$r)
  if(length(ml)>1){stop("The data set does not have the same map length in the burnin phase in every simulation.")}
  
  ml_expt = unique(data$sequence_length*data$r_expt)
  if(length(ml_expt)>1){stop("The data set does not have the same map length in the experiment in every simulation.")}
  
  end_gen = unique(data$end_gen)
  if(length(end_gen)>1){stop("The data doe not have have the same end_gen for every simulations")}
  
  if(end_gen==2){
    ml = unique(data$sequence_length*data$r_msp)
    if(length(ml)>1){stop("The data set does not have the same map length in the burnin phase in every simulation.")}
  }else{
    ml = unique(data$sequence_length*data$r)
    if(length(ml)>1){stop("The data set does not have the same map length in the burnin phase in every simulation.")}
  }
  
  mu_alpha = (data$shape)*(data$shape)
  
  title = ggdraw() + draw_label(paste("Map length in burnin = ", ml, " M, Map length in experiment = ", ml_expt, " M, scale = ", data$scale, sep = ""), hjust = 0.5, size = font_size)
  
  # Add title
  p_combined = plot_grid(title, p_combined, ncol = 1, rel_heights = c(0.1,1))
    
}

return(list(main = p_main, palpha = p_palpha, balpha_int = p_balpha_int, balpha_slope = p_balpha_slope,
            combined = p_combined))

  
}
```

Load the dataframe storing information on calculations of vA_true_new, va_true_new, va_lost, va_lost_new

```{r}
d_new_va = read.csv(file.path(data_path, "combined_data/new_va_calculation_combined.csv"))
alpha = TRUE # If alpha = TRUE, alpha = eta + (1 - 2p)eta is used to calculate vA_true, i.e. vA_true_new is used in the plots. Otherwise, vA_true is used in the plots
cumulative = FALSE # Should plots of va_lost vs cumulative |alpha| be mmade (time consuming)?
```


## Simplified simulations (simulations without burnin) (Set_5)

Selection coefficients were drawn from a gamma distribution (shape = 0.3, scale = 0.033) and assigned to mutations randomly. The recombination rate in the msprime simulation was 5.32e-07. Map length in the history phase of just 1 generation (eng_gen = 2) was 5.


Standard parameters:

Map length in the experiment phase (ml_exp) = 2 M
Number of individuals in the experiment (n_ind_exp) = 1000
Number of cages (n_cages) = 10
ngen2 = 4


```{r}
d_5 = read.csv(file.path(data_path, "combined_data/Set_5_output.csv"))
# Add calculations of vA_true_new, va_true_new, va_lost, va_lost_new
d_5 = add_new_va_data(d_new_va, d_5, TRUE)
d_5_std = d_5[d_5$r_expt*d_5$sequence_length==2&d_5$n_ind_exp==1000&d_5$n_cages==10&d_5$ngen2==4,] # Data for the standard set

##################################################################################################
####### Standard plots (Set_0: ml_expt = 2, n_indxpt = 1000, n_cages = 10, ngen2 = 4) ############
##################################################################################################

p_nb_2_std = make_std_plots(d_5_std, alpha = alpha)$combined
p_nb_2_std

title = ggdraw() + draw_label("Simplified simulations", hjust = 0.5, size = 30)
p_nb_2_std = plot_grid(title, p_nb_2_std, ncol = 1, rel_heights = c(0.1,1))

ggsave(p_nb_2_std, width = 8, height = 8, file = file.path(output_path, "Fig1.jpg"))
p_nb_2_std



```

We then vary ml_expt, n_ind_exp, n_cages, and ngen2 one at a time while keeping the other parameters fixed at their standard values.
```{r}
### ml_expt ###

d_5_ml = d_5[d_5$n_ind_exp==1000&d_5$n_cages==10&d_5$ngen2==4,]
p_nb_2_ml = ggplot(d_5_ml , aes(y=vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = factor(sequence_length*r_expt))) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)"), color = "Map length in\nmorgans \n(experiment)") + 
  scale_color_manual(values = c("#0072B2", "#999999", "#009E73")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE)
p_nb_2_ml

legend_nb_2_ml = get_legend(p_nb_2_ml)
p_nb_2_ml = plot_grid(p_nb_2_ml + theme(legend.position = "none"),
                      legend_nb_2_ml,
                      ncol = 2,
                      rel_widths = c(1.0, 0.3))
p_nb_2_ml

### n_ind_exp ###
d_5_nind = d_5[d_5$r_exp==2e-6&d_5$n_cages==10&d_5$ngen2==4,]
p_nb_2_nind = ggplot(d_5_nind , aes(y=vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = factor(n_ind_exp))) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)"), color = "Population \nsize \n(experiment)") + 
  scale_color_manual(values = c("#0072B2", "#999999", "#009E73")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) 
p_nb_2_nind

legend_nb_2_nind = get_legend(p_nb_2_nind)
p_nb_2_nind = plot_grid(p_nb_2_nind + theme(legend.position = "none"),
                        legend_nb_2_nind,
                        ncol = 2,
                        rel_widths = c(1.0, 0.3))
p_nb_2_nind

### n_cages ###

d_5_ncages = d_5[d_5$r_exp==2e-6&d_5$n_ind_exp==1000&d_5$ngen2==4,]
p_nb_2_ncages = ggplot(d_5_ncages , aes(y=vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = factor(n_cages))) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)"), color = "Replicate \npopulations") + 
  scale_color_manual(values = c("#0072B2", "#999999", "#009E73")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) 
p_nb_2_ncages

legend_nb_2_ncages = get_legend(p_nb_2_ncages)
p_nb_2_ncages = plot_grid(p_nb_2_ncages + theme(legend.position = "none"),
                          legend_nb_2_ncages,
                          ncol = 2,
                          rel_widths = c(1.0, 0.3))
p_nb_2_ncages

### ngen_expt ###

d_5_ngen2 = d_5[d_5$r_exp==2e-6&d_5$n_ind_exp==1000&d_5$n_cages==10,]
p_nb_2_ngen2 = ggplot(d_5_ngen2 , aes(y=vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = factor(ngen2-ngen1))) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)"), color = TeX(r"($\tau - t$)")) + 
  scale_color_manual(values = c("#0072B2", "#009E73", "#999999")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) 
p_nb_2_ngen2

legend_nb_2_ngen2 = get_legend(p_nb_2_ngen2)
p_nb_2_ngen2 = plot_grid(p_nb_2_ngen2 + theme(legend.position = "none"),
                         legend_nb_2_ngen2,
                         ncol = 2,
                         rel_widths = c(1.0, 0.3))
p_nb_2_ngen2

### Combine the plots ###

p_nb_2_combined = plot_grid(p_nb_2_ml, p_nb_2_nind, p_nb_2_ncages, p_nb_2_ngen2, labels = "AUTO")

title = ggdraw() + draw_label("Simplified simulations", hjust = 0.5, size = 30)

# Add title
p_nb_2_combined = plot_grid(title, p_nb_2_combined, ncol = 1, rel_heights = c(0.1,1))

p_nb_2_combined
ggsave(p_nb_2_combined, width = 12, height = 10, file = file.path(output_path, "Fig2.jpg"))
```



## Full simulations with a burnin phase of 25000 generations 


### Set 9 (larger simulations (scale = 0.033): ml = 0.5, ml_exp = 2)

The standard set for full simulations. 

Selection coefficients were drawn from a gamma distribution (shape = 0.3, scale = -0.033) and assigned to mutations randomly. No beneficial mutations. The forward simulation in the history phase was run for 25000 generations. The recombination rate in the msprime simulation was 5.32e-07. Map length in the history phase was 0.5 M, and the map length in the experiment was 2 M.

Number of individuals in the experiment (n_ind_exp) = 1000
Number of cages (n_cages) = 10
ngen2 = 4

The analyses for this set were performed using all.gp = FALSE and using the true N_E (i.e. using predict_Ne = TRUE)

```{r}
d_9 = read.csv(file.path(data_path, "combined_data/Set_9_output.csv")) # Data for the standard set

# Add calculations of vA_true_new, va_true_new, va_lost, va_lost_new
d_9 = add_new_va_data(d_new_va, d_9, TRUE)
d_9_std = d_9[d_9$all.gp==FALSE&d_9$Ne_exp!="1000_1000",]

p_b_large_std = make_std_plots(d_9_std, alpha = alpha)$combined
title = ggdraw() + draw_label("Full simulations", hjust = 0.5, size = 30)

p_b_large_std = plot_grid(title, p_b_large_std, ncol = 1, rel_heights = c(0.1,1))

ggsave(plot = p_b_large_std, width = 8, height = 8, file = file.path(output_path, "Fig3.jpg"))
p_b_large_std


```

We then vary various parameters one at a time while keeping the others fixed to their values for the previous set.

### Varying the ratio of beneficial:deleterious mutations (scale = 0.033)

```{r}
d_11 = read.csv(file.path(data_path, "combined_data/Set_11_output.csv")) # Data for the set with mut_ratio = 0.0002
# Add calculations of vA_true_new, va_true_new, va_lost, va_lost_new
d_11 = add_new_va_data(d_new_va, d_11, TRUE)
d_14 = read.csv(file.path(data_path, "combined_data/Set_14_output.csv")) # Data for the set with mut_ratio = 0.02
# Add calculations of vA_true_new, va_true_new, va_lost, va_lost_new
d_14 = add_new_va_data(d_new_va, d_14, TRUE)


p_b_large_ben_std = make_std_plots(d_11, alpha = FALSE)
p_b_large_ben_std$combined

# Combined plot for proportion of beneficial mutations

p_ben_0.033 = ggplot(rbind(d_9_std, d_11, d_14), aes(y = vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = factor(format(mut_ratio, scientific = FALSE)))) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)"), color = TeX(r"($\mu_{ben} : \mu_{del}$)")) + 
  scale_color_manual(values = c("#009E73", "#0072B2", "#999999")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) +
  theme(plot.title = element_text(hjust = 0.5))

p_ben_0.033

legend_ben = get_legend(p_ben_0.033)

p_ben_0.033 = plot_grid(p_ben_0.033 + theme(legend.position = "none"),
                        legend_ben,
                        ncol = 2,
                        rel_widths = c(1, 0.3))
p_ben_0.033
```

### Set_12 Varyingthe map length in the experiment phase (scale = 0.033): ml_exp = c(0.01, 0.2, 2), mut_ratio = 0

```{r}

d_12 = read.csv(file.path(data_path, "combined_data/Set_12_output.csv")) # Data when ml_exp was varied
# Add calculations of vA_true_new, va_true_new, va_lost, va_lost_new
d_12 = add_new_va_data(d_new_va, d_12, TRUE)

d_ml_exp = rbind(d_9_std, d_12)

p_ml_exp_0.033 = ggplot(d_ml_exp , aes(y=vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = factor(sequence_length*r_expt))) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)"), color = "Map length \nin morgans \n(experiment)") + 
  scale_color_manual(values = c("#0072B2", "#999999", "#009E73")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) +
  theme(plot.title = element_text(hjust = 0.5)) 


# When combining such plots, varying legend sizes can result in mesaligned plots
# Standardising the size of the plot and the space allocated to the legend by extractig the legend and then combining the legend-less plot and the legend in a standardised way

legend_ml_exp_0.033 = get_legend(p_ml_exp_0.033)
p_ml_exp_0.033 = plot_grid(p_ml_exp_0.033 + theme(legend.position = "none"),
                           legend_ml_exp_0.033,
                           ncol = 2,
                           rel_widths = c(1, 0.3))
p_ml_exp_0.033

```

## Simulations with burnin (varying the scale)

```{r}

d_15 = read.csv(file.path(data_path, "combined_data/Set_15_output.csv"))
# Add calculations of vA_true_new, va_true_new, va_lost, va_lost_new
d_15 = add_new_va_data(d_new_va, d_15, TRUE)


# Add calculations of vA_true_new, va_true_new, va_lost, va_lost_new

d_15 = add_new_va_data(d_new_va, d_15, TRUE)

d_scale = rbind(d_9_std, d_15)

d_scale = deduplicate_sim_analyses(d_scale)

# Deduplicate simulations. Some simulations were accidentally analysed twice

p_scale = ggplot(d_scale , aes(y=vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = factor(scale))) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)"), color = TeX(r"($\eta_{scale}$)")) + 
  scale_color_manual(values = c("#009E73", "#0072B2", "#999999")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) +
  theme(plot.title = element_text(hjust = 0.5)) 

# When combining such plots, varying legend sizes can result in mesaligned plots
# Standardising the size of the plot and the space allocated to the legend by extractig the legend and then combining the legend-less plot and the legend in a standardised way

legend_scale = get_legend(p_scale)

p_scale = plot_grid(p_scale + theme(legend.position = "none"),
                    legend_scale,
                    ncol = 2,
                    rel_widths = c(1, 0.3))

p_scale

```

## Simulations with burnin (varying the map length) (scale = 0.033)

```{r}

d_13 = read.csv(file.path(data_path, "combined_data/Set_13_output.csv"))
# Add calculations of vA_true_new, va_true_new, va_lost, va_lost_new
d_13 = add_new_va_data(d_new_va, d_13, TRUE)

# Combine with the standard data set

d_ml_history = rbind(d_9_std, d_13)

p_b_0.033_ml = ggplot(d_ml_history , aes(y=vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = factor(sequence_length*r))) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)"), color = "Map length \nin morgans \n(history)") + 
  scale_color_manual(values = c("#009E73", "#D01C8B", "#0072B2", "#999999")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) +
  theme(plot.title = element_text(hjust = 0.5))

# When combining such plots, varying legend sizes can result in mesaligned plots
# Standardising the size of the plot and the space allocated to the legend by extractig the legend and then combining the legend-less plot and the legend in a standardised way

legend_b_0.033_ml = get_legend(p_b_0.033_ml)
p_b_0.033_ml = plot_grid(p_b_0.033_ml + theme(legend.position = "none"),
                           legend_b_0.033_ml,
                           ncol = 2,
                           rel_widths = c(1, 0.3))

p_b_0.033_ml

```

## The standard set of full simulations (with burnin) (scale = 0.033) analysed with all.gp = FALSE (standard) or all.gp = TRUE (gametic phase LD not used)

```{r}

d_9_nstd = d_9[d_9$all.gp==TRUE&d_9$Ne_exp!="1000_1000",]

# In d_9_nstd there are some simulations that were analysed twiced
# Retaining among them the analysis performed first

d_9_nstd = deduplicate_sim_analyses(d_9_nstd)


d_all.gp = rbind(d_9_std, d_9_nstd)

all.gp_labs = c(TeX(r"(${\bf L}^{'}_{0}$ + $\frac{r_{ij}{\bf L}^{''}_{0}}{1-r_{ij}}$)"), TeX(r"($\bf L_0$)"))
names(all.gp_labs) = c(FALSE, TRUE)

p_all.gp = ggplot(d_all.gp, aes(y = vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = all.gp)) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)"), color = TeX(r"($\tilde{\bf L}_0$)")) + 
  scale_color_manual(values = c("#009E73", "#0072B2"), labels = all.gp_labs) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) +
  theme(plot.title = element_text(hjust = 0.5))

# When combining such plots, varying legend sizes can result in mesaligned plots
# Standardising the size of the plot and the space allocated to the legend by extractig the legend and then combining the legend-less plot and the legend in a standardised way

legend_all.gp = get_legend(p_all.gp)

p_all.gp = plot_grid(p_all.gp + theme(legend.position = "none"),
                     legend_all.gp,
                     ncol = 2,
                     rel_widths = c(1, 0.3))

p_all.gp



```

## The standard set of full simulations (with burnin) (scale = 0.033) analysed with Ne = nind = 1000

```{r}

d_9_fixed_Ne = d_9[d_9$all.gp==FALSE&d_9$Ne_exp=="1000_1000",]

d_9_Ne = rbind(d_9_std, d_9_fixed_Ne)

# Create labels for Ne_type
Ne_type = unique(d_9_Ne$Ne_exp)
Ne_type_labs = rep(NA, length(Ne_type))

for(i in 1:length(Ne_type)){
  Ne_type_labs[i] = round(as.numeric(unlist(strsplit(Ne_type[i], "_"))[2]), 2)
  nind_exp = unique(d_9_Ne[d_9_Ne$Ne_exp==Ne_type[i],]$n_ind_exp)
  
  if(Ne_type_labs[i]==nind_exp){
    Ne_type_labs[i] = paste("N:", Ne_type_labs[i])
  }else{
      Ne_type_labs[i] = paste("True:", Ne_type_labs[i])
      }
}

names(Ne_type_labs) = Ne_type

p_Ne = ggplot(d_9_Ne, aes(y = vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = Ne_exp)) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)"), color = TeX(r"($N_E$)")) + 
  scale_color_manual(values = c("#0072B2", "#009E73"), labels = Ne_type_labs) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) +
  theme(plot.title = element_text(hjust = 0.5))

legend_Ne = get_legend(p_Ne)

p_Ne = plot_grid(p_Ne + theme(legend.position = "none"),
                 legend_Ne,
                 ncol = 2,
                 rel_widths = c(1, 0.3))

# plot residual variance

p_Ne_res = ggplot(d_9_Ne, aes(y = Residual_var, x = Ne_exp)) +
  theme_bw() + 
  geom_boxplot() +
  labs(y = "Residual variance", x = TeX(r"($N_E$)")) +
  scale_x_discrete(labels = Ne_type_labs) +
  theme(text = element_text(size = 15))
  
ggsave(p_Ne_res, width = 5, height = 5, file = file.path(output_path, "FigS3.jpg"))

p_Ne
p_Ne_res


```


## Combine plots for large (scale = 0.033) full simulations (with burnin)

```{r}
p_b_0.033_combined = plot_grid(p_ml_exp_0.033, p_b_0.033_ml, p_ben_0.033, p_scale, p_all.gp, p_Ne, ncol = 2, labels = "AUTO")

title = ggdraw() + draw_label("Full simulations", hjust = 0.5, size = 30)

# Add title
p_b_0.033_combined = plot_grid(title, p_b_0.033_combined, ncol = 1, rel_heights = c(0.1,1))
p_b_0.033_combined

ggsave(p_b_0.033_combined, width = 12, height = 12, file = file.path(output_path, "Fig4.jpg"))


```

### Set_N1 (scale = 0.045): ml = c (5, 50, 250), ml_exp = 2, mut_ratio = 0 

Varying map length in history, but for \eta_{scale} = 0.045

```{r}
d_N1 = read.csv(file.path(data_path, "combined_data/Set_N1_output.csv"))
# Add calculations of vA_true_new, va_true_new, va_lost, va_lost_new
d_N1 = add_new_va_data(d_new_va, d_N1, TRUE)

### ml ###

p_b_0.045_ml = ggplot(d_N1 , aes(y=vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = factor(sequence_length*r))) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$)"), color = "Map length \nin morgans\n(history)", title = TeX(r"($\eta_{scale}$ = 0.045)")) + 
  scale_color_manual(values = c("#009E73", "#D01C8B", "#0072B2", "#999999")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) +
  theme(plot.title = element_text(hjust = 0.5))

p_b_0.045_ml

ggsave(p_b_0.045_ml, width = 6, height = 5, file = file.path(output_path, "FigS1.jpg"))

```

## Additive genic variance lost during the experiment

Plot the fraction of the initial additive genic variance lost by the end of the experiment as a function of non-neutral \eta_{scale}. Plot the error in our estimates of V_A as a function of the V_a lost in the simulations, colour-coded by \eta_{scale}. Then, identify the simulations with the greatest and smallest losses and plot locuswise V_a lost vs abs(\alpha) for these simulations. Finally, divide \alpha into intervals of 0.01, for each interval calculate the cumulative loss in V_a for the simulations with the minimum and maximum losses.

```{r va_lost}

##############
# Figure S2A #
##############

plot_lost_va = ggplot(d_scale, aes(y = if(alpha){((va_true_new - va_left_new)/va_true_new)}else{((va_true - va_left)/va_true)}, x = factor(round(scale, digits = 4)), fill = factor(round(scale, digits = 4)))) +
  theme_bw() +
  geom_boxplot() +
  labs(y = TeX(r"(Fraction of the total $V_a(0)$ lost)"), x = TeX(r"($\eta_{scale}$)"), fill  = TeX(r"($\eta_{scale}$)")) +
  theme(text = element_text(size = 15)) +
  scale_fill_manual(values = c("#009E73", "#0072B2", "#999999"))  +
  theme(legend.position = "bottom")

##############
# Figure S2B #
##############

# Does abs_va_lost explain the bias in our estimates?

plot_lost_va_bias = ggplot(d_scale, aes(y = if(alpha){vA_est - vA_true_new}else{vA_est - vA_true},
                                        x = if(alpha){va_true_new - va_left_new}else{va_true - va_left}, 
                                        color = factor(round(scale, digits = 4)))) +
  theme_bw() +
  geom_point() +
  labs(y = TeX(r"(Estimate of $V_A$ - True $V_A$)"),x = TeX(r"(Total $V_a$ lost)"), color = TeX(r"($\eta_{scale}$)")) +
  theme(text = element_text(size = 15)) +
  geom_abline(slope = -1, intercept = 0) + 
  scale_color_manual(values = c("#009E73", "#0072B2", "#999999"))  +
  theme(legend.position = "bottom")

##############
# Figure S2C #
##############

# Identify the simulation with the greatest absolute loss in V_a

if(alpha){
  max_loss_sim = d_scale[which((d_scale$va_true_new - d_scale$va_left_new) == max(d_scale$va_true_new - d_scale$va_left_new)),]$Set_ID}else{
  max_loss_sim = d_scale[which((d_scale$va_true - d_scale$va_left) == max(d_scale$va_true - d_scale$va_left)),]$Set_ID  
  }

message("Simulation with the geratest absolute loss in initial additive genic variance:")
max_loss_sim
max_loss_sim_vA_true = if(alpha){d_scale[d_scale$Set_ID==max_loss_sim,]$vA_true_new}else{d_scale[d_scale$Set_ID==max_loss_sim,]$vA_true}
message(paste("True V_A =", max_loss_sim_vA_true))

message(paste("Lost V_a =", if(alpha){d_scale[d_scale$Set_ID==max_loss_sim,]$va_true_new - d_scale[d_scale$Set_ID==max_loss_sim,]$va_left_new}else{d_scale[d_scale$Set_ID==max_loss_sim,]$va_true - d_scale[d_scale$Set_ID==max_loss_sim,]$va_left}))

# Identify the simulation with the smallest absolute loss in V_a
# However, limit only to simulations for which vA_true (or vA_true_new) is close to max_loss_sim_vA_true

d_scale1 = d_scale[if(alpha){which((max_loss_sim_vA_true - 0.001) <= d_scale$vA_true_new & d_scale$vA_true_new<= (max_loss_sim_vA_true + 0.001))}else{which((max_loss_sim_vA_true - 0.001) <= d_scale$vA_true & d_scale$vA_true_new <= (max_loss_sim_vA_true + 0.001))},]

if(alpha){
  min_loss_sim = d_scale1[which((d_scale1$va_true_new - d_scale1$va_left_new) == min(d_scale1$va_true_new - d_scale1$va_left_new)),]$Set_ID}else{
  min_loss_sim = d_scale1[which((d_scale1$va_true - d_scale1$va_left) == min(d_scale1$va_true - d_scale1$va_left)),]$Set_ID  
  }
message("Simulation with the smallest absolute loss in initial additive genic variance:")
min_loss_sim
message(paste("True V_A =", if(alpha){d_scale[d_scale$Set_ID==min_loss_sim,]$vA_true_new}else{d_scale[d_scale$Set_ID==min_loss_sim,]$vA_true}))
message(paste("Lost V_a =", if(alpha){d_scale[d_scale$Set_ID==min_loss_sim,]$va_true_new - d_scale[d_scale$Set_ID==min_loss_sim,]$va_left_new}else{d_scale[d_scale$Set_ID==min_loss_sim,]$va_true - d_scale[d_scale$Set_ID==min_loss_sim,]$va_left}))

# Load finescale data for these two simulations

# List names of files containing finescale data for all sims

finescale_file_list = list.files(pattern = ".*finescale.*.csv$", full.names = TRUE, path = data_path)

d_finescale_max <- read.csv(finescale_file_list[grepl(max_loss_sim, finescale_file_list)][1])
d_finescale_min <- read.csv(finescale_file_list[grepl(min_loss_sim, finescale_file_list)][1])

# Calculate locuswise_va_lost

d_finescale_max$locuswise_va_lost = d_finescale_max$locuswise_va_true - d_finescale_max$locuswise_va_left
d_finescale_max$locuswise_va_lost_new = d_finescale_max$locuswise_va_true_new - d_finescale_max$locuswise_va_left_new

d_finescale_min$locuswise_va_lost = d_finescale_min$locuswise_va_true - d_finescale_min$locuswise_va_left
d_finescale_min$locuswise_va_lost_new = d_finescale_min$locuswise_va_true_new - d_finescale_min$locuswise_va_left_new

if(cumulative){
  
  # Sort by abs|\alpha| and calculate cumulative locuswise_va_lost_new
  
    d_finescale_max = d_finescale_max[order(abs(d_finescale_max$list_alpha_new)),]
    d_finescale_min = d_finescale_min[order(abs(d_finescale_min$list_alpha_new)),]
    
    d_finescale_max$cumulative_va_lost_new = NA
    for(i in 1:nrow(d_finescale_max)){
      d_finescale_max$cumulative_va_lost_new[i] = sum(d_finescale_max$locuswise_va_lost_new[1:i])
    }
    
    d_finescale_min$cumulative_va_lost_new = NA
    for(i in 1:nrow(d_finescale_min)){
      d_finescale_min$cumulative_va_lost_new[i] = sum(d_finescale_min$locuswise_va_lost_new[1:i])
    }
    
  # Sort by abs|\eta| and calculate cumulative locuswise_va_lost  
  
    d_finescale_max = d_finescale_max[order(abs(d_finescale_max$list_alpha)),]
    d_finescale_min = d_finescale_min[order(abs(d_finescale_min$list_alpha)),]
    
    d_finescale_max$cumulative_va_lost = NA
    for(i in 1:nrow(d_finescale_max)){
     d_finescale_max$cumulative_va_lost[i] = sum(d_finescale_max$locuswise_va_lost[1:i])
    }
    
    d_finescale_min$cumulative_va_lost = NA
    for(i in 1:nrow(d_finescale_min)){
      d_finescale_min$cumulative_va_lost[i] = sum(d_finescale_min$locuswise_va_lost[1:i])
    }
  
}

d_finescale = rbind(d_finescale_max, d_finescale_min)

d_finescale$locuswise_va_lost = d_finescale$locuswise_va_true - d_finescale$locuswise_va_left
d_finescale$locuswise_va_lost_new = d_finescale$locuswise_va_true_new - d_finescale$locuswise_va_left_new

d_finescale$lost_va_type = ifelse(d_finescale$Set_ID==min_loss_sim, "Min.", "Max.")


# Restricting to selected loci only
d_finescale_sel = d_finescale[d_finescale$list_alpha!=0,]

plot_finescale_va_lost = ggplot(d_finescale_sel, aes(y = if(alpha){locuswise_va_lost_new}else{locuswise_va_lost},                                                      x = if(alpha){abs(list_alpha_new)}else{abs(list_alpha)}, 
                                                     color = lost_va_type)) +
  theme_bw() +
  geom_point() +
  labs(y = TeX(r"($V_a$ lost at a locus$)"), x = if(alpha){TeX(r"($|\alpha|$)")}else{TeX(r"($|\eta|$)")}, color = "Simulation type") +
  theme(text = element_text(size = 15)) +   
  theme(legend.position = "bottom") +
  scale_color_manual(values = c("Min." = "#009E73", "Max." = "#999999"),
    labels = c("Min." = TeX(r"(Min. $V_a$ lost)"), "Max." = TeX(r"(Max. $V_a$ lost)"))  # Custom legend labels
  ) +
  geom_line(aes(x = seq(0, 1.2, length = nrow(d_finescale_sel)), y = 0.5*(1/2000)*(1999/2000)*seq(0, 1.2, length = nrow(d_finescale_sel))^2), linewidth = 0.1, color = "black")

plot_finescale_va_lost  

if(cumulative){
    
  # Plot cumulative V_a lost vs |alpha| for the two simulations
  
  plot_cumulative_va_lost = ggplot(d_finescale_sel, aes(y = if(alpha){cumulative_va_lost_new}else{cumulative_va_lost},                                                      x = if(alpha){abs(list_alpha_new)}else{abs(list_alpha)}, 
                                                       color = lost_va_type)) +
    theme_bw() +
    geom_point() +
    labs(y = TeX(r"(Cumulative $V_a$ lost$)"), x = if(alpha){TeX(r"($|\alpha|$)")}else{TeX(r"($|\eta|$)")}, color = "Simulation type") +
    theme(text = element_text(size = 15)) +   
    theme(legend.position = "bottom") +
    scale_color_manual(values = c("Min." = "#009E73", "Max." = "#999999"),
      labels = c("Min." = TeX(r"(Min. $V_a$ lost)"), "Max." = TeX(r"(Max. $V_a$ lost)"))  # Custom legend labels
    ) 
  plot_cumulative_va_lost
  
}


##############
# Figure S2D #
##############

d_finescale_sel$list_alpha_cat = abs(round(d_finescale_sel$list_alpha, 2))
d_finescale_sel$list_alpha_new_cat = abs(round(d_finescale_sel$list_alpha_new, 2))

if(alpha){
  d_summed_lost_va = aggregate(cbind(locuswise_va_true_new, locuswise_va_lost_new) ~ list_alpha_new_cat*lost_va_type, data = d_finescale_sel, FUN = "sum")
  
  d_summed_lost_va$fract_va_lost = d_summed_lost_va$locuswise_va_lost_new/d_summed_lost_va$locuswise_va_true_new
  
}else{
  d_summed_lost_va = aggregate(cbind(locuswise_va, locuswise_va_lost) ~ list_alpha_cat*lost_va_type, data = d_finescale_sel, FUN = "sum")
}


plot_categoriwise_va_lost = ggplot(d_summed_lost_va, aes(y = locuswise_va_lost_new,                                                      x = if(alpha){list_alpha_new_cat}else{list_alpha_cat}, 
                                                       fill = lost_va_type)) +
    theme_bw() +
    geom_bar(stat = "Identity", position = position_dodge(preserve = "single")) +
    labs(y = TeX(r"(Total $V_a$ lost at groups of loci)"), x = if(alpha){TeX(r"($|\alpha|$)")}else{TeX(r"($|\eta|$)")}, fill = "Simulation type") +
    theme(text = element_text(size = 15)) +   
    theme(legend.position = "bottom") +
    scale_fill_manual(values = c("Min." = "#009E73", "Max." = "#999999"),
      labels = c("Min." = TeX(r"(Min. $V_a$ lost)"), "Max." = TeX(r"(Max. $V_a$ lost)"))  # Custom legend labels
    ) 
  plot_categoriwise_va_lost
  
##############
# Figure S2E #
##############
  
  plot_categoriwise_fract_va_lost = ggplot(d_summed_lost_va, aes(y = fract_va_lost,                                                      x = if(alpha){list_alpha_new_cat}else{list_alpha_cat}, 
                                                       fill = lost_va_type)) +
    theme_bw() +
    geom_bar(stat = "Identity", position = position_dodge(preserve = "single")) +
    labs(y = TeX(r"(Fraction $V_a$ lost at groups of loci)"), x = if(alpha){TeX(r"($|\alpha|$)")}else{TeX(r"($|\eta|$)")}, fill = "Simulation type") +
    theme(text = element_text(size = 15)) +   
    theme(legend.position = "bottom") +
    scale_fill_manual(values = c("Min." = "#009E73", "Max." = "#999999"),
      labels = c("Min." = TeX(r"(Min. $V_a$ lost)"), "Max." = TeX(r"(Max. $V_a$ lost)"))  # Custom legend labels
    ) 
  plot_categoriwise_fract_va_lost
  
  


### Combine the three plots ###

va_lost_combined_plot = plot_grid(plot_lost_va, plot_lost_va_bias, plot_finescale_va_lost, plot_categoriwise_va_lost, ncol = 2, nrow = 2, labels = "AUTO")

ggsave(va_lost_combined_plot, width = 10, height = 10, file = file.path(output_path, "FigS2.jpg"))

### Calculate the the fraction of total amount of V_a lost at loci for alpha > 0.3 in the "Max." (V_a lost) simulation

message("The fraction of V_a lost in the 'Max.' simulation at loci with alphas larger than 0.3 = ")

if(alpha){
  sum(d_summed_lost_va[d_summed_lost_va$lost_va_type=="Max."&d_summed_lost_va$list_alpha_new_cat>0.3,]$locuswise_va_lost_new)/sum(d_summed_lost_va[d_summed_lost_va$lost_va_type=="Max.",]$locuswise_va_lost_new)
}else{
  sum(d_summed_lost_va[d_summed_lost_va$lost_va_type=="Max."&d_summed_lost_va$list_alpha_cat>0.3,]$locuswise_va_lost)/sum(d_summed_lost_va[d_summed_lost_va$lost_va_type=="Max.",]$locuswise_va_lost)
}

message("The fraction of non-neutral loci in the 'Max.' simulation with alphas larger than 0.3 = ")

if(alpha){
  nrow(d_finescale_sel[d_finescale_sel$lost_va_type=="Max."&d_finescale_sel$list_alpha_new>0.3,])/nrow(d_finescale_sel[d_finescale_sel$lost_va_type=="Max.",])
}else{
   nrow(d_finescale_sel[d_finescale_sel$lost_va_type=="Max."&d_finescale_sel$list_alpha>0.3,])/nrow(d_finescale_sel[d_finescale_sel$lost_va_type=="Max.",]) 
  }

message("Mean |alpha| for non neutral loci in the 'Max.' simulation = ")

if(alpha){
    mean(abs(d_finescale_sel[d_finescale_sel$lost_va_type=="Max.",]$list_alpha_new))
}else{
    mean(abs(d_finescale_sel[d_finescale_sel$lost_va_type=="Max.",]$list_alpha))
  }


```

## Calculating V_A using Buffalo and Coop's (2019) method

```{r}
d_bc = read.csv(file.path(data_path, "combined_data/BC_V_A_corrected_combined.csv"))

# Create a new data frame for simplified simulations with estimates using different BC approaches one below the other

d_bc_smpl = data.frame("vA_BC" = c(d_bc[d_bc$end_gen==2,]$vA_BC_1, d_bc[d_bc$end_gen==2,]$vA_BC_2, d_bc[d_bc$end_gen==2,]$vA_BC_3),
                       "vA_true" = rep(d_bc[d_bc$end_gen==2,]$vA_true, 3),
                       "vA_true_new" = rep(d_bc[d_bc$end_gen==2,]$vA_true_new, 3),
                       "BC_approach" = c(rep("Approach 1", nrow(d_bc[d_bc$end_gen==2,])), rep("Approach 2", nrow(d_bc[d_bc$end_gen==2,])), rep("Approach 3", nrow(d_bc[d_bc$end_gen==2,]))))

p_bc_smpl = ggplot(d_bc_smpl, aes(x = if(alpha){vA_true_new}else{vA_true}, y = vA_BC, color = BC_approach)) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$ using B&C)"), color = " ") + 
  scale_color_manual(values = c("#0072B2", "#999999", "#009E73")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE)

p_bc_smpl

d_bc_full = data.frame("vA_BC" = c(d_bc[d_bc$end_gen==25000,]$vA_BC_1, d_bc[d_bc$end_gen==25000,]$vA_BC_2, d_bc[d_bc$end_gen==25000,]$vA_BC_3),
                       "vA_true" = rep(d_bc[d_bc$end_gen==25000,]$vA_true, 3),
                       "vA_true_new" = rep(d_bc[d_bc$end_gen==25000,]$vA_true_new, 3),
                       "BC_approach" = c(rep("Approach 1", nrow(d_bc[d_bc$end_gen==25000,])), rep("Approach 2", nrow(d_bc[d_bc$end_gen==25000,])), rep("Approach 3", nrow(d_bc[d_bc$end_gen==25000,]))))

p_bc_full = ggplot(d_bc_full, aes(x = if(alpha){vA_true_new}else{vA_true}, y = vA_BC, color = BC_approach)) +
  theme_bw() + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$ using B&C)"), color = " ") + 
  scale_color_manual(values = c("#0072B2", "#999999", "#009E73")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE)

p_bc_full


```


## Relationship between addetive genic variance lost and B_alpha(1) and p_alpha

Miscellaneous plots.

```{r}

#######

d_ml_palpha_balpha = data.frame("palpha" = c(d_ml_history$palpha_emp, d_ml_history$palpha_est),
                       "balpha_slope" = c(d_ml_history$balpha_slope_emp, d_ml_history$balpha_slope_est),
                       "type" = c(rep("True", nrow(d_ml_history)), rep("Estimate", nrow(d_ml_history))),
                       "r" = c(d_ml_history$r, d_ml_history$r),
                       "sequence_length" = c(d_ml_history$sequence_length, d_ml_history$sequence_length))

p_ml_palpha = ggplot(d_ml_palpha_balpha, aes(y = palpha, x = factor(r*sequence_length), fill = type)) +
  theme_bw() +
  geom_boxplot() + labs(y = TeX(r"($p_{\alpha} $)")) +
  theme(text = element_text(size = 15)) +
  labs(x = "Map length in morgans (history)", fill = "")
p_ml_palpha

p_ml_balpha_slope = ggplot(d_ml_palpha_balpha, aes(y = balpha_slope, x = factor(r*sequence_length), fill = type)) +
  theme_bw() +
  geom_boxplot() + labs(y = TeX(r"($\beta^{(1)}_{\bar{\alpha}}$)"), fill = "") +
  theme(text = element_text(size = 15)) +
  labs(x = "Map length in morgans (history)")
p_ml_balpha_slope

p_ml_diversity = ggplot(d_ml_history, aes(y = mean_diversity, x = factor(r*sequence_length))) +
  theme_bw() +
  geom_boxplot() + labs(y = "Mean diversity at segregating sites") +
  theme(text = element_text(size = 15)) +
  labs(x = "Map length in morgans (history)")
p_ml_diversity



p_ml_issues = plot_grid(p_ml_palpha, 
                        p_ml_balpha_slope, 
                        p_ml_diversity, 
                        nrow = 2, labels = "AUTO")

ggsave(p_ml_issues, file = file.path(output_path, "ml_issues.jpg"), w = 9, h = 9)

p_ml_seg_sites_sel = ggplot(d_ml_history, aes(y = seg_sites-seg_sites_neu, x = vA_true_new, color = factor(r*sequence_length))) +
  theme_bw() +
  geom_point() + labs(y = "Number of non-neutral segregating sites") +
  theme(text = element_text(size = 15)) +
  labs(color = "Map length in morgans (history)", x = TeX(r"(True $V_A$)")) + theme(legend.position = "bottom") +
  scale_color_manual(values = c("#009E73", "#D01C8B", "#0072B2", "#999999"))
p_ml_seg_sites_sel

p_ml_diversity_vA = ggplot(d_ml_history, aes(y = mean_diversity, x = vA_true_new, color = factor(r*sequence_length))) +
  theme_bw() +
  geom_point() + labs(y = "Mean diversity at segregating sites") +
  theme(text = element_text(size = 15)) +
  labs(color = "Map length in morgans (history)", x = TeX(r"(True $V_A$)")) + theme(legend.position = "bottom") +
  scale_color_manual(values = c("#009E73", "#D01C8B", "#0072B2", "#999999"))
p_ml_diversity_vA

### Separate plot of diversity vs vA_true for neutral and non-neutral sites

d_diversity_ml = data.frame("Set_ID" = rep(d_ml_history$Set_ID, 2), "vA_true_new" = rep(d_ml_history$vA_true_new, each = 2), "locus_type" = c(rep(c("selected", "neutral"), nrow(d_ml_history))), "diversity" = rep(NA, 2*nrow(d_ml_history)))

count = 0
for(set_id in d_ml_history$Set_ID){
  count = count + 1
  # Read finecsale data
  print(set_id)
  d = read.csv(list.files(pattern = paste(set_id, ".*finescale.*.csv$", sep = ""), full.names = TRUE, path = data_path)[1])
  
  d_selected = d[d$list_alpha!=0,]
  d_neutral = d[d$list_alpha==0,]
  
  # Calculate selected and neutral diversities
  
  pi_selected = mean(0.5*d_selected$pbar0*(1-d_selected$pbar0))
  pi_neutral = mean(0.5*d_neutral$pbar0*(1-d_neutral$pbar0))
  
  d_diversity_ml$diversity[2*count - 1] = pi_selected
  d_diversity_ml$diversity[2*count] = pi_neutral
  
  message(paste(count/nrow(d_ml_history)*100), "% done")
}

write.table(d_diversity_ml, file = file.path(output_path, "ml_diversity.csv"))

# make figure

d_diversity_ml$r = rep(d_ml_history$r, each = 2)
d_diversity_ml$sequence_length = rep(d_ml_history$sequence_length, each = 2)
d_diversity_ml$vA_true_new = rep(d_ml_history$vA_true_new, each = 2)

p_ml_diversity_vA_separate = ggplot(d_diversity_ml, aes(y = diversity, x = vA_true_new, shape = locus_type, color = factor(r*sequence_length))) +
  theme_bw() +
  geom_point(size = 2) + labs(y = "Mean diversity at segregating sites") +
  theme(text = element_text(size = 15)) +
  labs(color = "ML (history)", x = TeX(r"(True $V_A$)")) + theme(legend.position = "bottom") +
  scale_color_manual(values = c("#009E73", "#D01C8B", "#0072B2", "#999999"))
p_ml_diversity_vA_separate

p_ml_issues_2 = plot_grid(p_ml_seg_sites_sel, p_ml_diversity_vA, p_ml_diversity_vA_separate, nrow = 3)
ggsave(p_ml_issues_2, file = file.path(output_path, "ml_issues_2.jpg"), w = 8, h = 15)



```

## Sanity check

To perform sanity checks on our method, and our implementation of BC's method, we ran (simplified, i.e. no burnin) simulations with free recombination in the experiment (r_exp = 0.005). The recombiantion rate in the history (including msprime) was either 5e-7 (the default) or 1e-5.

```{r}
d_sanity = read.csv(file.path(data_path, "combined_data/sanity_check_output.csv"))
d_sanity = add_new_va_data(d_new_va, d_sanity)
d_sanity$vA_BC_est_actual = unlist(lapply(d_sanity$vA_BC, function(x){as.numeric(unlist(strsplit(x, "_"))[3])}))
d_sanity$vA_BC_est_exact = unlist(lapply(d_sanity$vA_BC, function(x){as.numeric(unlist(strsplit(x, "_"))[7])}))
d_sanity$assumption_G_test = unlist(lapply(d_sanity$vA_BC, function(x){as.numeric(unlist(strsplit(x, "_"))[12])}))

p_sanity_us = ggplot(d_sanity, aes(y = vA_est, x = if(alpha){vA_true_new}else{vA_true}, color = as.factor(sequence_length*r_msp))) +
  theme_bw() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$ using our method)"), color = "Map length \nin history") + 
  scale_color_manual(values = c("#009E73", "#D01C8B", "#0072B2", "#999999")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) + ylim(0, 0.35) + xlim(0, 0.13)

p_sanity_bc_actual = ggplot(d_sanity, aes(y = vA_BC_est_actual, x = if(alpha){vA_true_new}else{vA_true}, color = as.factor(sequence_length*r_msp))) +
  theme_bw() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$ using B&C (actual))"), color = "Map length \nin history") + 
  scale_color_manual(values = c("#009E73", "#D01C8B", "#0072B2", "#999999")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE)  + ylim(0, 0.35) + xlim(0, 0.13)

p_sanity_bc_exact = ggplot(d_sanity, aes(y = vA_BC_est_exact, x = if(alpha){vA_true_new}else{vA_true}, color = as.factor(sequence_length*r_msp))) +
  theme_bw() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) + 
  labs(x = TeX(r"(True $V_A$)"), y = TeX(r"(Estimate of $V_A$ using B&C (improved))"), color = "Map length \nin history") + 
  scale_color_manual(values = c("#009E73", "#D01C8B", "#0072B2", "#999999")) + 
  theme(text = element_text(size = 15)) +
  geom_smooth(method = "lm", se=FALSE) + ylim(0, 0.35) + xlim(0, 0.13)

p_sanity_assumption_G_test = ggplot(d_sanity, aes(y = assumption_G_test, x = as.factor(sequence_length*r_msp), fill = as.factor(sequence_length*r_msp))) +
  theme_bw() +
  geom_boxplot() + 
  labs(y = "Deviation from Assumption G in B&C", x = "Map length in history") + 
  scale_fill_manual(values = c("#009E73", "#D01C8B", "#0072B2", "#999999")) + 
  theme(text = element_text(size = 15)) 

legend_sanity = get_legend(p_sanity_us)

p_sanity_combined = plot_grid(p_sanity_bc_actual + theme(legend.position = "none"), 
                              p_sanity_bc_exact + theme(legend.position = "none"), 
                              p_sanity_us + theme(legend.position = "none"), 
                              p_sanity_assumption_G_test + theme(legend.position = "none"), ncol = 2, labels = "AUTO")
p_sanity_combined = plot_grid(p_sanity_combined, legend_sanity, ncol = 2, rel_widths = c(1, 0.15))

ggsave(p_sanity_combined, file = file.path(output_path, "sanity.jpg"), w = 9, h = 9)



```

